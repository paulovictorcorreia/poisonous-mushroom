{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f697c4-76e5-4492-adff-b3c155813c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import joblib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import yaml\n",
    "import gdown\n",
    "import os\n",
    "import pprint\n",
    "import zipfile\n",
    "from sklearn.metrics import make_scorer \n",
    "from typing import List, Text\n",
    "import matplotlib.colors\n",
    "# https://drive.google.com/file/d/1v5PlKhhafsmWEvRUBj4Zpo2kkNDDR4HY/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07f933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paulo\\Documents\\Projects\\poisonous_mushroom\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a37453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base': {'n_jobs': -1, 'random_state': 42},\n",
      " 'data_decompress': {'processed_path': 'data/processed'},\n",
      " 'data_load': {'compact_name': 'mushroom.zip',\n",
      "               'file_id': '1v5PlKhhafsmWEvRUBj4Zpo2kkNDDR4HY',\n",
      "               'raw_path': 'data/raw'},\n",
      " 'evaluate': None,\n",
      " 'metadata': {'id_col': 'id', 'target_col': 'class'},\n",
      " 'preprocessing': {'original_test': 'data/processed/test.csv',\n",
      "                   'original_train': 'data/processed/train.csv',\n",
      "                   'test_size': 0.2,\n",
      "                   'testset_path': 'data/interim/test.csv',\n",
      "                   'trainset_path': 'data/interim/train.csv',\n",
      "                   'valset_path': 'data/interim/val.csv'},\n",
      " 'train': {'model_path': 'models/model.joblib',\n",
      "           'random_forest': {'max_depth': 5, 'n_estimators': 20}}}\n"
     ]
    }
   ],
   "source": [
    "with open('params.yaml') as conf_file:\n",
    "    config = yaml.safe_load(conf_file)\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e2f7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_path = config['data_load']['raw_path']\n",
    "# filename = config['data_load']['compact_name']\n",
    "# file_id = config['data_load']['file_id']\n",
    "\n",
    "\n",
    "# def download_compact_file(config_path: str) -> None:\n",
    "#     with open(config_path) as conf_file:\n",
    "#         config = yaml.safe_load(conf_file)\n",
    "#     raw_data_path = config['data_load']['raw_path']\n",
    "#     filename = config['data_load']['compact_name']\n",
    "#     file_id = config['data_load']['file_id']\n",
    "#     download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "#     output = os.path.join(raw_data_path, filename)\n",
    "#     print(f'Downloading to {output}')\n",
    "#     gdown.download(download_url, output, quiet=False)\n",
    "\n",
    "\n",
    "\n",
    "# download_compact_file('params.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7697e27b",
   "metadata": {},
   "source": [
    "# Importing and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b81984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to data/raw\\mushroom.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1v5PlKhhafsmWEvRUBj4Zpo2kkNDDR4HY\n",
      "From (redirected): https://drive.google.com/uc?id=1v5PlKhhafsmWEvRUBj4Zpo2kkNDDR4HY&confirm=t&uuid=a83d7b9c-2717-4d7e-8707-c7832bdc3c9d\n",
      "To: c:\\Users\\Paulo\\Documents\\Projects\\poisonous_mushroom\\data\\raw\\mushroom.zip\n",
      "100%|██████████| 86.3M/86.3M [00:16<00:00, 5.14MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets\n"
     ]
    }
   ],
   "source": [
    "def extract_datasets(config_path: str) -> None:\n",
    "    with open(config_path) as conf_file:\n",
    "        config = yaml.safe_load(conf_file)\n",
    "    raw_data_path = config['data_load']['raw_path']\n",
    "    filename = config['data_load']['compact_name']\n",
    "    file_id = config['data_load']['file_id']\n",
    "    download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    output = os.path.join(raw_data_path, filename)\n",
    "    print(f'Downloading to {output}')\n",
    "    gdown.download(download_url, output, quiet=False)\n",
    "\n",
    "    print('Extracting datasets')\n",
    "    zip_file_path = os.path.join(raw_data_path, filename)\n",
    "    processed_path = config['data_decompress']['processed_path']\n",
    "    os.makedirs(processed_path, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(processed_path)\n",
    "\n",
    "\n",
    "extract_datasets('params.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac60e13d",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1415adcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cap-shape cap-surface cap-color does-bruise-or-bleed gill-attachment  \\\n",
      "id                                                                             \n",
      "1252551         s           w         n                    t               d   \n",
      "1799166         b           g         o                    f             NaN   \n",
      "1936146         x           i         o                    f               e   \n",
      "1464811         f         NaN         y                    f               s   \n",
      "767639          f         NaN         n                    f               d   \n",
      "\n",
      "        gill-spacing gill-color stem-root stem-surface stem-color veil-type  \\\n",
      "id                                                                            \n",
      "1252551            c          n       NaN          NaN          n       NaN   \n",
      "1799166            c          n       NaN          NaN          n       NaN   \n",
      "1936146          NaN          y       NaN          NaN          k       NaN   \n",
      "1464811            d          y       NaN            i          y       NaN   \n",
      "767639             c          y         b          NaN          n       NaN   \n",
      "\n",
      "        veil-color has-ring ring-type spore-print-color habitat season  \n",
      "id                                                                      \n",
      "1252551        NaN        f         f               NaN       d      a  \n",
      "1799166        NaN        f         f               NaN       m      a  \n",
      "1936146        NaN        f         f               NaN       m      a  \n",
      "1464811        NaN        f         f               NaN       d      a  \n",
      "767639         NaN        f         f               NaN       l      w  \n"
     ]
    }
   ],
   "source": [
    "def split_and_preprocess_data(config: Text) -> None:\n",
    "\n",
    "    with open(config) as conf_file:\n",
    "        config = yaml.safe_load(conf_file)\n",
    "\n",
    "    train_file = config['preprocessing']['original_train']\n",
    "    test_file = config['preprocessing']['original_test']\n",
    "    id_column = config['metadata']['id_col']\n",
    "    target_column = config['metadata']['target_col']\n",
    "\n",
    "    train_data = pd.read_csv(train_file, index_col=id_column)\n",
    "    train_data.drop(columns=['stem-height', 'stem-width', 'cap-diameter'], inplace=True)\n",
    "    \n",
    "\n",
    "    test_data = pd.read_csv(test_file, index_col=id_column)\n",
    "    test_data.drop(columns=['stem-height', 'stem-width', 'cap-diameter'], inplace=True)\n",
    "\n",
    "\n",
    "    train_data[target_column] = train_data[target_column].replace({'e':0.0, 'p':1.0})\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        train_data.drop(target_column, axis=1), train_data[target_column],\n",
    "        test_size=config['preprocessing']['test_size'],\n",
    "        random_state=config['base']['random_state']\n",
    "    )\n",
    "\n",
    "    columns = x_train.columns.to_list()\n",
    "    idx_train = x_train.index\n",
    "    idx_val = x_val.index\n",
    "    idx_test = test_data.index\n",
    "\n",
    "\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    x_train = imputer.fit_transform(x_train)\n",
    "    x_val = imputer.transform(x_val)\n",
    "    x_test = imputer.transform(test_data)\n",
    "\n",
    "    ordinal_encoding = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    x_train = ordinal_encoding.fit_transform(x_train)\n",
    "    x_val = ordinal_encoding.transform(x_val)\n",
    "    x_test = ordinal_encoding.transform(x_test)\n",
    "\n",
    "    x_train = pd.DataFrame(x_train, columns=columns, index=idx_train)\n",
    "    x_val = pd.DataFrame(x_val, columns=columns, index=idx_val)\n",
    "    x_test = pd.DataFrame(x_test, columns=columns, index=idx_test)\n",
    "\n",
    "    train_data = pd.concat([x_train, y_train], axis=1)\n",
    "    val_data = pd.concat([x_val, y_val], axis=1)\n",
    "\n",
    "    train_data.to_csv(config['preprocessing']['trainset_path'])\n",
    "    val_data.to_csv(config['preprocessing']['valset_path'])\n",
    "    x_test.to_csv(config['preprocessing']['testset_path'])\n",
    "\n",
    "    pass\n",
    "\n",
    "split_and_preprocess_data('params.yaml')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa0bb3e",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e3394c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def train_model(config: Text):\n",
    "\n",
    "    with open(config) as conf_file:\n",
    "        config = yaml.safe_load(conf_file)\n",
    "\n",
    "    train_data = pd.read_csv(config['preprocessing']['trainset_path'],\n",
    "                             index_col=config['metadata']['id_col'])\n",
    "\n",
    "    x_train = train_data.drop(\"class\", axis=1)\n",
    "    y_train = train_data['class']\n",
    "\n",
    "\n",
    "\n",
    "    rfc = RandomForestClassifier(\n",
    "        n_estimators=config['train']['random_forest']['n_estimators'],\n",
    "        max_depth=config['train']['random_forest']['max_depth'],\n",
    "        random_state=config['base']['random_state'],\n",
    "        n_jobs=config['base']['n_jobs']\n",
    "    )\n",
    "\n",
    "    rfc.fit(x_train, y_train)\n",
    "\n",
    "    models_path = config['train']['model_path']\n",
    "\n",
    "    joblib.dump(rfc, models_path)\n",
    "\n",
    "    pass\n",
    "\n",
    "train_model('params.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3888d",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4c08155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(config: Text) -> None:\n",
    "    with open(config) as conf_file:\n",
    "        config = yaml.safe_load(conf_file)\n",
    "\n",
    "    model_path = config['train']['model_path']\n",
    "    model = joblib.load(model_path)\n",
    "    train_file = config['preprocessing']['trainset_path']\n",
    "    val_file = config['preprocessing']['valset_path']\n",
    "    id_column = config['metadata']['id_col']\n",
    "    target_column = config['metadata']['target_col']\n",
    "\n",
    "\n",
    "\n",
    "    train_data = pd.read_csv(train_file, index_col=id_column)\n",
    "    x_train = train_data.drop(target_column, axis=1)\n",
    "    y_train = train_data[target_column].values\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "    \n",
    "\n",
    "    val_data = pd.read_csv(val_file, index_col=id_column)\n",
    "    x_val = val_data.drop(target_column, axis=1)\n",
    "    y_val = val_data[target_column].values\n",
    "    y_pred = model.predict(x_val)\n",
    "    f1_val = f1_score(y_val, y_pred)\n",
    "\n",
    "    report = {\n",
    "        'train':{\n",
    "            'f1': f1_train,\n",
    "            # 'actual': list(y_train),\n",
    "            # 'predicted': list(y_train_pred)\n",
    "        },\n",
    "        'val':{\n",
    "            'f1': f1_val,\n",
    "            # 'actual': list(y_val),\n",
    "            # 'predicted': list(y_pred)\n",
    "        }\n",
    "    }\n",
    "    report_path = os.path.join(config['evaluate']['reports_dir'], config['evaluate']['metrics_file'])\n",
    "    with open(report_path, 'w+') as file:\n",
    "        json.dump(report, file)\n",
    "\n",
    "\n",
    "evaluate('params.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd2add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm: np.array,\n",
    "                          target_names: List[Text],\n",
    "                          title: Text = 'Confusion matrix',\n",
    "                          cmap: matplotlib.colors.LinearSegmentedColormap = None,\n",
    "                          normalize: bool = True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "\n",
    "    return plt.gcf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
